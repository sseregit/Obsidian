# 1. 마이크로서비스를 쿠버네티스에 배포할 때 유레카 서버를 제거하는 이유는 무엇인가?
- kubernets에는 기본적으로 디스커버리 서비스가 내장되어 있다. 따라서 Netflix Eureka와 같은 별도의 디스커버리 서비스를 배포할 필요가 없다.
# 2. 무엇으로 유레카 서버를 대체했는가? 이 과정에서 마이크로서비스의 소스 코드는 어떤 영향을 받았는가?
- Eureka 서버는 Kubernetes의 내장된 Service 객체와 kube-proxy 런타임 컴포넌트로 대체되었습니다.
- 마이크로서비스의 소스코드는 이 변경으로 영향을 받지 않았습니다. 다만 다음과 같은 빌드 의존성과 설정 변경이 있었다.
	- 모든 `build.gradle` 파일에서 `spring-cloud-starter-netflix-eureka-client` 의존성 제거
	- config-repo 저장소에서 Eureka 관련 설정 제거
	- 모든 통합 테스트에서 더 이상 필요하지 않은 `eureka.client.enabled=false` 설정 제거
	- `@LoadBalanced` WebClient 에서 제거
# 3. 라이브니스 프로브와 레디니스 프로브의 사용 목적은 무엇인가?
- `Liveness probe`
	- Kubernetes가 컨테이너를 재시작해야 할지를 판단하는 데 사용
- `Readiness probe`
	- Kubernetes가 컨테이너가 외부 요청을 수락할 준비가 되었는지를 판단하는 데 사용
# 4. 스프링 부트의 단계적 종료 메니즘이 유용한 이유는 무엇인가?
- graceful shutdown을 적용하면 마이크로서비스는 새 요청을 더 이상 수락하지 않고, 설정된 시간 동안 현재 처리 중인 요청이 완료되기를 기다린 후 종료됩니다. 이 시간보다 오래 걸리는 요청은 중단된다.
# 5. 다음 헬름 템플릿 지시문의 사용 목적은 무엇인가?
```yaml
{{- $common := dict "Values" .Values.common -}}
{{- $noCommon := omit .Values "common" -}}
{{- $overrides := dict "Values" $noCommon -}}
{{- $noValues := omit . "Values" -}}
{{- with merge $noValues $overrides $common -}}
```
- 부모 차트가 사용하는 하위 차트의 값을 오버라이드할 수 있도록 오버라이드 메커니즘을 구현한다.
# 6. 다음의 명명된 헬름 템플릿이 실패하는 이유는 무엇인가?
```yaml
{{- define "common.secrets" -}}
{{- ragne $secretName, $secreMap := .Values.secrets -}}
apiVersion: v1
kind: Secret
metadata:
	name: {{ $secretName }}
	labels:
		app.kubernetes.io/name: {{ $secretName }}
type: Opaque
data:
{{- range $key, $val := $secretMap }}
	{{ $key }}: {{ $val | b64enc }}
{{- end }}
{{- end -}}
{{- end -}}
```
- `.Values.secrets`의 각 항목에 대해 Secret 매니페스트를 생성하지만, YAML에서 여러 매니페스트를 구분할 때 필요한 `---` 구분자가 없다. 마지막 두 개의 end앞에 `---` 줄을 추가해야 한다.
# 7. 다음의 매니페스트가 작동하지 않는 이유는 무엇인가?
```yaml
apiVersion: v1
kind: Service
metadata:
	name: review
	labels:
		app.kubernetes.io/name: review
spec:
	type: ClusterIP
	ports:
		- name: http
		  port: 80
		  protocol: TCP
		  targetPort: http
	selector:
		app.kubernetes.io/pod-name: review
---
apiVersion: apps/v1
kind: Deployment
metadata:
	name: review
	labels:
		app.kubernetes.io/name: review
spec:
	replicas: 1
	selector:
		matchLabels:
			app.kubernetes.io/name: review
	template:
		metadata:
			labels:
				app.kubernets.io/name: review
		spec:
			containers:
				- name: review
				  image: "hands-on/review-service:latest"
				  ports:
					- containerPort: 80
					  name: http-port
					  protocol: TCP
```
- Deployment에서 생성된 Pod에는 app.kubernetes.io/name=review 라벨이 붙는다. 그러나 Service는 app.kubernetes.io/pod-name=review 라벨을 기준으로 selector를 사용한다. 이 불일치로 인해 Service가 Pod를 선택할 수 없다.
- Service의 selector 라벨 키를 app.kubernetes.io/name으로 수정해야 한다.
# 8. 어떤 일이 발생했는가?
`CrashLoopBackOff`
- Zipkin 서버는 시작 시점에 RabbitMQ에 접근해야 하고, 이때 접근에 실패하면 Crash가 발생하고 Kubernetes가 이를 감지해 자동으로 재시작한다. 두 번째 시작시에는 RabbitMQ가 기동되어 있어 Zipkin도 정상 기동된다.